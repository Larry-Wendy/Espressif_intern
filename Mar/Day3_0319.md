# 3.19 学习记录
---
| 内容         | 时间        |
| ------------ | ----------- |
| Git安装配置  | 10:00-11:00 |
| Git基本命令学习 | 11:00-11:30 |
| Git与Github互传 | 14:00-15:00 |
| 学习Markdown   | 15:00-15:30 |
| 记录学习过程   | 15:30-17:00 |
---
## 人工智能原理 
### 1. 罗森布拉特感知机
*如何用一个函数来模拟人脑中的认知？*  
罗森布拉特(Rosenblatt)构想了感知机，它作为简化的数学模型解释大脑神经元如何工作：它取一组二进制输入值（附近的神经元），将每个输入值乘以一个连续值权重（每个附近神经元的突触强度），并设立一个阈值，如果这些加权输入值的和超过这个阈值，就输出1，否则输出0（同理于神经元是否放电）。并且增加了一个对AI而言至关重要的学习机制，以简单的一元一次函数 $y=wx$ 为例：
  * 初始化权重参数 $w$
  * 计算响应 $y'=wx$
  * 计算误差 $e=y-y'$
  * 自适应调整权重参数 $w'=w+\alpha·e·x$ ($\alpha$为学习率)
  * 回到第2步
  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" width="600" />
  </div>
  <p align="center">
  图1. 感知机结构
  </p>
 
### 2. 方差代价函数
*如何更好地衡量误差？*
 * 绝对误差：$|y-y'|$ 数学上不好处理
 * 均方误差：$(y-y')^2$ √  
若采用平方误差，误差与权重的关系为开口向上的一元二次函数。我们希望最后的误差最小，即需要找到一个权重$w$，其对应误差函数的极小值。

### 3. 梯度下降与反向传播
*如何得到我们想要的权重$w$呢？*
* 直接公式计算：在数据量偏大时需要巨大的计算量和存储量
* 梯度下降+反向传播：依据大量训练样本数据反复迭代更新权值参数，使其向最低点靠近。比如方差代价函数为 $e=aw^2+bw+c$，可以用$w_new = w - \alpha·k$来更新权重。用代价函数去修正预测函数参数的过程就是反向传播。
  * 批量梯度下降：所有样本合成为一个总体的代价函数，对合成代价函数进行梯度下降，得到全局最优点。但对于海量数据占用计算资源过大。
  * 随机梯度下降：每次对单个样本进行梯度下降，虽然会有振荡和波动，但总体也会向全局最优点靠近。但无法并行计算，得不到非常理想的结果。
  * Mini-batch梯度下降：折中考虑前两种方法，每次选取全局样本中的一小批进行梯度下降

如果增加一个偏置项$b$，$y=wx+b$，就需要学习两个参数。其代价函数就是三维平面中的一个曲面，对$w$和$b$分别求偏导数，并把结果看作向量，其和向量就是曲面内该点下降最快的方向。所以“梯度”是比“斜率”更广泛的概念。梯度下降与反向传播的过程见图2。
  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E6%84%9F%E7%9F%A5%E5%99%A8%E8%AE%AD%E7%BB%83.PNG" width="600" />
  </div>
  <p align="center">
  图2. 梯度下降与反向传播简单流程
  </p>

### 4. 激活函数
*如何实现分类？*
人脑处理的更多是分类问题，而不是对一件事物的精准预测，所以感知器输出的结果需要用激活函数进行分类。同时激活函数给系统带来了非线性，为神经网络增加了生物学特性。
* 阶跃函数作为激活函数的问题在于分界点处的导数不好处理，并且其他位置的导数均为0无法进行梯度下降。
* Sigmoid函数：$S(x)=\frac{1}{1+e^{-x}}$  
它很圆润且导数处处不为0，代价函数现在变为 $e=(y-sigmoid(wx+b))^2$，用复合函数求导链式法则得到结果，进行梯度下降。  
然而Sigmoid函数也存在其局限性，当曲线向两侧扩展时会变得非常平缓，此时导数无限趋近于0，梯度消失。但对于二分类的问题仍可以作为最后一层进行分类。
* ReLU函数：$R(x)=max(0,z)$  
更加有效率的梯度下降以及反向传播，避免了梯度爆炸和梯度消失问题。虽然在小于0的情况下会出现梯度死亡，但经验证明ReLU函数作为隐藏层的激活函数综合效果最好。RELU函数的引入给神经网络增加了生物学特性，可以称为灵魂激活函数。  
* Softmax函数：$S_i=\frac{e^i}{\sum{e^j}}$

### 5. 隐藏层与全连接层
*如何处理更复杂的问题？*
单个的神经元最后处理的结果只能是单调的（激活函数就是单调函数），为了能处理更复杂的问题，比如需要产生山丘一样的预测曲线，就需要增加神经元让神经元形成网络。  
添加一个神经元相当于增添一个抽象维度，每个维度通过调整权重并激活来得到对输入的不同理解。最后合并降维实现更复杂的分类效果。图3就是增加了一个由两个神经元组成的隐藏层的简单神经网络，通常隐藏层数超过3层就可以看作深度神经网络。 
由于图3中的隐藏层中的每个神经元均与上一层的每个输入和下一层的每个输出相连，故又称为全连接层。
  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.PNG" width="600" />
  </div>
  <p align="center">
  图3. 简单神经网络
  </p>
