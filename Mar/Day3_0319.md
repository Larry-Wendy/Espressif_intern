# 3.19 学习记录
---
## 人工智能原理 
### 1. 罗森布拉特感知机
*如何用一个函数来模拟人脑中的认知？*  
罗森布拉特(Rosenblatt)构想了感知机，它作为简化的数学模型解释大脑神经元如何工作：它取一组二进制输入值（附近的神经元），将每个输入值乘以一个连续值权重（每个附近神经元的突触强度），并设立一个阈值，如果这些加权输入值的和超过这个阈值，就输出1，否则输出0（同理于神经元是否放电）。并且增加了一个对AI而言至关重要的学习机制，以简单的一元一次函数 $y=wx$ 为例：
  * 初始化权重参数 $w$
  * 计算响应 $y'=wx$
  * 计算误差 $e=y-y'$
  * 自适应调整权重参数 $w'=w+\alpha·e·x$ ($\alpha$为学习率)
  * 回到第2步
 
  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E6%84%9F%E7%9F%A5%E6%9C%BA.png" width="600" />
  </div>
  <p align="center">
  图1. 感知机结构
  </p>
 
### 2. 代价函数
*如何更好地衡量误差？*
 * 绝对误差：$|y-y'|$ 数学上不好处理
 * 均方误差：$(y-y')^2$ √  
若采用平方误差，误差与权重的关系为开口向上的一元二次函数。我们希望最后的误差最小，即需要找到一个权重$w$，其对应误差函数的极小值。
 * 交叉熵：$-\sum{ylog(y')}$  
均方误差不适合做分类问题，因为它是一个非凸函数，容易陷入局部最优，不利于找到相对的全局最优解。而交叉熵衡量信息的不确定性的，也就是说熵越大，这个网络的输出不确定性就越大，也就是说网络没学到东西，反知，交叉熵越小，网络输出不确定性越小，估计越准确，网络学到了东西。同时它对参数求导结果就不受激活函数的导数影响了，而是受到激活函数输出值的影响。

### 3. 梯度下降与反向传播
*如何得到我们想要的权重$w$呢？*
* 直接公式计算：在数据量偏大时需要巨大的计算量和存储量
* 梯度下降+反向传播：依据大量训练样本数据反复迭代更新权值参数，使其向最低点靠近。比如方差代价函数为 $e=aw^2+bw+c$，可以用$w_new = w - \alpha·k$来更新权重。用代价函数去修正预测函数参数的过程就是反向传播。
  * 批量梯度下降：所有样本合成为一个总体的代价函数，对合成代价函数进行梯度下降，得到全局最优点。但对于海量数据占用计算资源过大。
  * 随机梯度下降：每次对单个样本进行梯度下降，虽然会有振荡和波动，但总体也会向全局最优点靠近。但无法并行计算，得不到非常理想的结果。
  * Mini-batch梯度下降：折中考虑前两种方法，每次选取全局样本中的一小批进行梯度下降

如果增加一个偏置项$b$，$y=wx+b$，就需要学习两个参数。其代价函数就是三维平面中的一个曲面，对$w$和$b$分别求偏导数，并把结果看作向量，其和向量就是曲面内该点下降最快的方向。所以“梯度”是比“斜率”更广泛的概念。梯度下降与反向传播的过程见图2。

  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E6%84%9F%E7%9F%A5%E5%99%A8%E8%AE%AD%E7%BB%83.PNG" width="600" />
  </div>
  <p align="center">
  图2. 梯度下降与反向传播简单流程
  </p>

### 4. 激活函数
*如何实现分类？*
人脑处理的更多是分类问题，而不是对一件事物的精准预测，所以感知器输出的结果需要用激活函数进行分类。同时激活函数给系统带来了非线性，为神经网络增加了生物学特性。
* 阶跃函数作为激活函数的问题在于分界点处的导数不好处理，并且其他位置的导数均为0无法进行梯度下降。
* Sigmoid函数：$S(x)=\frac{1}{1+e^{-x}}$  
它很圆润且导数处处不为0，代价函数现在变为 $e=(y-sigmoid(wx+b))^2$，用复合函数求导链式法则得到结果，进行梯度下降。  
然而Sigmoid函数也存在其局限性，当曲线向两侧扩展时会变得非常平缓，此时导数无限趋近于0，梯度消失。但对于二分类的问题仍可以作为最后一层进行分类。
* ReLU函数：$R(x)=max(0,z)$  
更加有效率的梯度下降以及反向传播，避免了梯度爆炸和梯度消失问题。虽然在小于0的情况下会出现梯度死亡（leakyReLU就是针对梯度死亡提出的），但经验证明ReLU函数作为隐藏层的激活函数综合效果最好。RELU函数的引入给神经网络增加了生物学特性，可以称为灵魂激活函数。  
* Softmax函数：$S_i=\frac{e^i}{\sum{e^j}}$  
Sigmoid一般用于二分类问题，而SoftMax用于多分类问题。因为Softmax层可以把最终输出的多个概率值归一化成有一个最大值（最终预测值），并且多个输出和为1的样式，可作为预测结果与one-hot编码的标签数据进行代价函数计算。  

  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.jpg" width="600" />
  </div>
  <p align="center">
  图3. 各类激活函数
  </p>

### 5. 隐藏层与全连接层
*如何处理更复杂的问题？*
单个的神经元最后处理的结果只能是单调的（激活函数就是单调函数），为了能处理更复杂的问题，比如需要产生山丘一样的预测曲线，就需要增加神经元让神经元形成网络。  
添加一个神经元相当于增添一个抽象维度，每个维度通过调整权重并激活来得到对输入的不同理解。最后合并降维实现更复杂的分类效果。图3就是增加了一个由两个神经元组成的隐藏层的简单神经网络，通常隐藏层数超过3层就可以看作深度神经网络。 
由于图3中的隐藏层中的每个神经元均与上一层的每个输入和下一层的每个输出相连，故又称为全连接层。

  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.PNG" width="600" />
  </div>
  <p align="center">
  图4. 简单神经网络
  </p>

### 6. 数据处理
* 划分训练集，验证集和测试集。划分比例依数据量来定，但一般可先8:2划分训练集和测试集，再从训练集抽取0.1-0.2作为验证集。
  * 训练集：用于模型拟合的数据样本
  * 验证集：是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 通常用来在模型迭代训练时，用以验证当前模型泛化能力（准确率，召回率等），以决定是否停止继续训练
  * 测试集：用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据  
* 归一化操作：当输入值相差过大时，代价函数的曲面过窄，梯度下降困难。如果将输入数据归一化到[0,1]之间，梯度下降会比较容易。
  
### 7. 性能评估
| 效果         | 训练集效果        | 测试集效果  |
| ------------ | ----------- | ------------ |
| Prefect      | 好           | 好           |
| 几乎不存在    | 坏           | 好          |
| 欠拟合       | 坏            | 坏          |
| 过拟合       | 好            | 坏          |

* 过拟合：很可能是用过分复杂的模型去拟合简单的问题，类似于“死记硬背”“钻牛角尖”。解决方法可以考虑①调整结构 ②l2正则化 ③节点失活

### 8. 卷积神经网络
*如何更好地针对二维图像信息进行分类？*
仅用全连接层的深度神经网络可以进行图像分类，把二维的像素矩阵转化为一维的向量送入网络就行了，提高性能需要的就是大量的神经元节点和超强的计算能力，但这显然只是“暴力求解”的方式，我们需要一个模型性能的突破。  
回到人眼观察的场景，我们会主动提取图片的轮廓、花纹、颜色等信息，而这些信息只有在二维时是有意义的。当像素被分配到一维，许多特征会混淆甚至消失。如何提取二维的特征呢？仅靠全连接层是做不到的。
* 卷积层：卷积核对像素矩阵进行卷积
* 参数共享：复用同同一套卷积核，大大减小了参数量。同时由于图像各位置提取特征的方式是一样的，所以参数共享是有意义的。
* 通道数：数据的第三个维度。比如RGB数据有红绿蓝三层，通道数就是3。如果经过4个卷积核卷积提取4个特征，通道数变为4。
* 池化层：降低数据量，提取主要特征。有平均池化，最大池化等。
* 填充层：避免卷积导致的数据量快速下降，可以在卷积之前填充一圈0，保持矩阵大小不变。
经典的LeNet-5卷积神经网络如下图所示：

  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/LeNet-5.PNG" width="600" />
  </div>
  <p align="center">
  图5. LeNet-5卷积神经网络
  </p>
 
### 9. RNN和LSTM：自然语言处理
*如何更好处理时域上前后关联的数据？*
细想BP算法,卷积神经网络我们会发现, 他们的输出都是只考虑前一个输入的影响而不考虑其它时刻输入的影响, 比如简单的猫,狗,手写数字等单个物体的识别具有较好的效果. 但是, 对于一些与时间先后有关的, 比如视频的下一时刻的预测,文档前后文内容的预测等, 这些算法的表现就不尽如人意了.因此, RNN就应运而生了。  
* RNN(循环神经网络):   
  RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中。  
  但RNN仍然存在着一些问题, 其中较为严重的是容易出现梯度消失或者梯度爆炸的问题(BP算法和长时间依赖造成的)，即对长依赖问题的表现不佳。
* LSTM(长短记忆网络):
  LSTM可以通过“门”结构来去除或者增加“细胞状态”的信息,实现了对重要内容的保留和对不重要内容的去除。  
  用于遗忘的门叫做"遗忘门"，用于信息增加的叫做"信息增加门"，后是用于输出的"输出门"。
* GRU(门控循环单元):
  GRU是2014年提出的一种LSTM改进算法。它将忘记门和输入门合并成为一个单一的更新门, 同时合并了数据单元状态和隐藏状态, 使得模型结构比之于LSTM更为简单。
  
  <div align=center>
  <img src="https://github.com/Larry-Wendy/Espressif_intern/blob/main/fig/RNN_LSTM_GRU.webp" width="600" />
  </div>
  <p align="center">
  图6. RNN,LSTM和GRU
  </p>


